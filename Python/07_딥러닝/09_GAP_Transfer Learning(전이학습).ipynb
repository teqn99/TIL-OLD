{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"position":{"height":"551.4px","left":"1166px","right":"20px","top":"120px","width":"350px"},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"09_GAP_Transfer Learning(전이학습).ipynb","provenance":[],"collapsed_sections":["P0xjY4JNNt0N","IouksDx5Nt0O"],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"GGEEitm2Nt0B"},"source":["# GlobalAveragePooling (GAP)\n","- Feature map의 채널별로 평균값을 추출 1 x 1 x channel 의 Feature map을 생성\n","- `model.add(keras.layers.GlobalAveragePooling2D())`\n","![image-2.png](attachment:image-2.png)"]},{"cell_type":"markdown","metadata":{"id":"RT5FmyDRNt0I"},"source":["- Feature Extraction layer에서 추출한 Feature map을 Classifier layer로 Flatten해서 전달하면 많은 연결노드와 파라미터가 필요하게된다. GAP를 사용하면 노드와 파라미터의 개수를 효과적으로 줄일 수 있다.\n","- Feature map의 채널수가 많을 경우 GAP를 사용하는 것이 효과적이나 채널수가 적다면 Flatten을 사용하는 것이 좋다.\n","![image-2.png](attachment:image-2.png)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":103},"id":"wwoL2kbRNt0J","executionInfo":{"status":"ok","timestamp":1619599427339,"user_tz":-540,"elapsed":3294,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"0d91d43f-9256-4a88-ec1c-c0c3e04c6629"},"source":["# 이미지 다운로드\n","import gdown\n","url = 'https://drive.google.com/uc?id=1nBE3N2cXQGwD8JaD0JZ2LmFD-n3D5hVU'\n","fname = 'cats_and_dogs_small.zip'\n","gdown.download(url, fname, quiet=False)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1nBE3N2cXQGwD8JaD0JZ2LmFD-n3D5hVU\n","To: /content/cats_and_dogs_small.zip\n","90.8MB [00:00, 105MB/s] \n"],"name":"stderr"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'cats_and_dogs_small.zip'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"qWySU8dYNt0K","executionInfo":{"status":"ok","timestamp":1619599427340,"user_tz":-540,"elapsed":1701,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}}},"source":["!mkdir data"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"rKj9R395Nt0K","executionInfo":{"status":"ok","timestamp":1619599429398,"user_tz":-540,"elapsed":3602,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}}},"source":["## 압축 풀기\n","!unzip -q ./cats_and_dogs_small.zip -d data/cats_and_dogs_small"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"4KEoUlwiOFuS","executionInfo":{"status":"ok","timestamp":1619599430026,"user_tz":-540,"elapsed":3919,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}}},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","def get_generators():\n","    '''\n","    train, validation, test generator를 생성해서 반환.\n","    train generator는 image 변환 처리\n","    '''\n","    train_dir = './data/cats_and_dogs_small/train'\n","    validation_dir = './data/cats_and_dogs_small/validation'\n","    test_dir = './data/cats_and_dogs_small/test'\n","    train_datagen = ImageDataGenerator(rescale=1/255,\n","                                       rotation_range=40,\n","                                       brightness_range=(0.7,1.3),\n","                                       zoom_range=0.2,\n","                                       horizontal_flip=True)\n","    test_datagen = ImageDataGenerator(rescale=1/255) #validation/test에서 사용\n","    # generator 들 생성\n","    train_generator = train_datagen.flow_from_directory(train_dir,\n","                                                        target_size=(150,150),\n","                                                        batch_size=N_BATCHS,\n","                                                        class_mode='binary')\n","    val_generator = test_datagen.flow_from_directory(validation_dir,\n","                                                        target_size=(150,150),\n","                                                        batch_size=N_BATCHS,\n","                                                        class_mode='binary')\n","    test_generator = test_datagen.flow_from_directory(test_dir,\n","                                                        target_size=(150,150),\n","                                                        batch_size=N_BATCHS,\n","                                                        class_mode='binary')\n","    return train_generator, val_generator, test_generator"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PfESNY1jNt0L"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"bJ-PmxAWNt0L"},"source":["## Transfer learning (전이학습)\n","- 큰 데이터 셋을 이용해 미리 학습된 pre-trained Model의 Weight를 사용하여 현재 하려는 예측 문제에 활용. \n","- ### Convolution base(Feature Extraction 부분)만 활용\n","    - Convolution base는 이미지에 나타나는 일반적인 특성을 파악하기 위한 부분이므로 재사용할 수 있다.\n","    - Classifier 부분은 학습하려는 데이터셋의 class들에 맞게 변경 해야 하므로 재사용할 수 없다.\n","- Pretrained Convlution layer의 활용 \n","    - Feature extraction\n","        - 학습시 학습되지 않고 Feature를 추출하는 역할만 한다.\n","    - Fine tuning\n","        - 학습시 Pretrained Covolution layer도 같이 학습해서 내 데이터셋에 맞춘다."]},{"cell_type":"markdown","metadata":{"id":"MibERrnLNt0L"},"source":["## Feature extraction\n","- 기존의 학습된 network에서 fully connected layer를 제외한 나머지 weight를 고정하고 새로운 목적에 맞는 fully connected layer를 추가하여 추가된 weight만 학습하는 방법\n","- `tensorflow.keras.applications` module이 지원하는  image classification models\n","    - (https://www.tensorflow.org/api_docs/python/tf/keras/applications)    \n","![image.png](attachment:image.png)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3ussEvxpNt0M"},"source":["> ### ImageNet\n",">    - 웹상에서 수집한 약 1500만장의 라벨링된 고해상도 이미지로 약 22,000개 카테고리로 구성된다.\n","\n","> ### ILSVRC(ImageNet Large Scale Visual Recognition Challenge) 대회\n",">   - 2010년 부터 2017년 까지 진행된 컴퓨터 비전 경진대회.\n",">   - ImageNet의 이미지중 **1000개 카테고리 약 120만장의 학습용이미지, 5만장의 검증 이미지, 15만장의 테스트 이미지를** 이용해 대회를 진행한다.\n",">   - **2012년** CNN기반 딥러닝 알고리즘인 **AlexNet**이 2위와 큰 차이로 우승하며 이후 딥러닝 알고리즘이 대세가 되었다. 특히 2015년 우승한 ResNet은 0.036의 에러율을 보이며 우승했는데 이는 사람이 에러율이라 알려진 0.05 보다 높은 정확도였다.\n",">   - ILSVRC에서 우승하거나 좋은 성적을 올린 모델들이 컴퓨터 비전분야 발전에 큰 역할을 해왔으며 이후 다양한 딥러닝 모델의 백본(backbone)으로 사용되고 있다.\n","\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"ex38uPvpNt0M"},"source":["##  VGG16 모델\n","- ImageNet ILSVRC Challenge 2014에서 2등한 모델로 Simonyan and Zisserman(Oxford Univ.)에 의해 제안\n","    - VGGNet이 준우승을 하긴 했지만, 구조의 간결함과 사용의 편이성으로 인해 1등한 GoogLeNet보다 더 각광받았다\n","- 단순한 구조로 지금까지 많이 사용.\n","- 총 16개 layer로 구성됨.\n","- 네트워크 깊이가 어떤 영향을 주는 지 연구 하기 위해 설계된 네트워크로 동일한 kernel size에 convolution의 개수를 늘리는 방식으로 구성됨.\n","    - 11 layer, 13 layer, 16 layer, 19 layer 의 네트워크를 테스트함. \n","    - 19 layer의 성능이 16 layer보다 크게 나아지지 않음\n","- Filter의 수가 64, 128, 256, 512 두 배씩 커짐 \n","- 항상 $3 \\times 3$ filter, Stride=1, same padding, $2\\times 2$ MaxPooling 사용\n","    - 이전 AlexNet이 5 X 5 필터를 사용했는데 VGG16은 3 X 3 필터 두개를 쌓아 사용했다.\n","        - 3 x 3 필터 두개를 쌓는 것이 5 x 5  하나는 사용하는 보다 더 적은 파라미터를 사용하며 성능이 더 좋았다.\n","    - Feature map의 사이즈를 convolution layer가 아닌 Max Pooling 을 사용해 줄여줌.\n","- VGG16의 단점은 마지막에 분류를 위해 Fully Connected Layer 3개를 붙여 파라미터 수가 너무 많아 졌다. 약 1억4천만 개의 parameter(가중치)중 1억 2천만개 정도가 Fully Connected Layer의 파라미터 임.\n","![image-3.png](attachment:image-3.png)"]},{"cell_type":"markdown","metadata":{"id":"3gufID5MNt0M"},"source":["## ResNet (Residual Networks)\n","- 이전 모델들과 비교해 shortcut connection기법을 이용해 Layer수를 획기적으로 늘린 CNN 모델로 ILSVRC 2015년 대회에서 우승을 차지함.\n","\n","![image.png](attachment:image.png)\n","\n","- 레이어를 깊게 쌓으면 성능이 더 좋아 지지 않을까? 실제는 Test 셋 뿐만 아니라 Train Set에서도 성능이 나쁘게 나옴.\n","- Train set에서도 성능이 나쁘게 나온 것은 최적화 문제로 보고, 레이어를 깊게 쌓으면 최적화 하기가 어렵다고 생각함. \n","![image-2.png](attachment:image-2.png)"]},{"cell_type":"markdown","metadata":{"id":"7IPKX9fWNt0N"},"source":["### Idea\n","![image-3.png](attachment:image-3.png)"]},{"cell_type":"markdown","metadata":{"id":"3L2enl9FNt0N"},"source":["- 입력값을 그대로 출력하는 identity block 을 사용하면 성능이 떨어지지는 않는다.\n","- 그럼 Convloution block을 identity block으로 만들면 최소한 성능은 떨어지지 않고 깊은 Layer를 쌓을 수 있지 않을까?"]},{"cell_type":"markdown","metadata":{"id":"5djI1GW3Nt0N"},"source":["### Solution\n","- Residual block\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"LbzGf2frNt0N"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"P0xjY4JNNt0N"},"source":["- 기존 Layer들의 목표는 입력값인 X를 출력값인 Y로 최적의 매핑할 수 있는 함수 H(X)를 찾는 것이다. 그래서 H(X) – Y 가 최소값이 되는 방향으로 학습을 진행하면서 H(X)를 찾음. 그런데 레이어가 깊어지면서 최적화에 어려움으로 성능이 떨어지는 문제가 발생\n","\n","- ResNet은 layer를 통과해서 나온 값이 **입력값과 동일하게 만드는 것을 목표로 하는 Identity block을** 구성한다.\n","- Identity block은 입력값 X를 레이어를 통과시켜서 나온 Y에 입력값 X를 더해서 합치도록 구성한다.\n","\n","$$\\large H(x) = F(x) + x\\\\x: input,\\;H(x): output,\\;F(x): layer통과값$$ \n"," \n"," \n","- 목표는 $H(x)$(레이어통과한 값) 가 input인 x와 동일한 것이므로 F(x)를 0으로 만들기 위해 학습을 한다. \n","- $F(x)$는 **잔차(Residual)**가 된다. 그리고 잔차인 $F(x)$가 0이 되도록 학습하는 방식이므로 Residual Learning이라고 한다.\n","- 입력인 x를 직접 전달하는 것을 **shortcut connection** 또는 **identity mapping** 또는 **skip connection** 이라고 한다.\n","    - 이 shortcut은 파라미터 없이 단순히 값을 더하는 구조이므로 연산량에 크게 영향이 없다.\n","- 그리고 Residual을 찾는 레이어를 **Residual Block, Identity Block** 이라고 한다.      \n","\n","### 성능향상\n","- $H(x) = F(x) + x$ 을 $x$에 대해 미분하면 최소한 1이므로 Gradient Vanishing 문제를 극복한다.\n","- 잔차학습이라고 하지만 Residual block 은 Convolution Layer와 Activation Layer로 구성되어 있기 때문에 이 Layer를 통과한 Input으로 부터 Feature map을 추출하는 과정은 진행되며 레이어가 깊으므로 다양한 더욱 풍부한 특성들을 추출하게 되어 성능이 향상된다.\n","  "]},{"cell_type":"markdown","metadata":{"id":"IouksDx5Nt0O"},"source":["### ResNet 구조\n","- Residual block들을 쌓는 구조\n","    - 일반 Convolution Layer(backbone)을 먼저 쌓고 Identity(Residual) block들을 계속 쌓는다.\n","- 모든 Identity block은 두개의 3X3 conv layer로 구성됨.\n","- 일정 레이어 수별로 filter의 개수를 두배로 증가시키며 stride를 2로 하여 downsampling 함. (Pooling Layer는 Identity block의 시작과 마지막에만 적용)\n","![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"BwRAgF_DNt0O"},"source":["![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"wpTZUonvNt0O"},"source":["## Pretrained Model 사용\n","- tensorflow.keras.applications 패키지를 통해 제공\n","- 모델이름이 클래스이름\n","    - VGG16, ResNet153 등등\n","- 생성자 매개변수\n","    - `weights`: 모형의 학습된 weight. 기본값- 'imagenet'\n","    - `include_top`: fully connected layer를 포함할지 여부. True 포함시킴, False: 포함 안 시킴\n","    - `input_shape`: 사용자가 입력할 이미지의 크기 shape. 3D 텐서로 지정. (높이, 너비, 채널). 기본값: (224,224, 3)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1pdTXWH4Ox1M","executionInfo":{"status":"ok","timestamp":1619599460382,"user_tz":-540,"elapsed":2146,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"e1114a4f-be32-4e77-c558-67b35235f903"},"source":["from tensorflow.keras.applications import VGG16, ResNet50V2\n","\n","# conv_base = VGG16(weights='imagenet',  # imagenet 데이터셋을 학습한 가중치(파라미터) 사용 - imagenet(default)\n","#                   include_top=False,  # Classfication(Fully Connected Layer)는 가져오지 않겠다.(False) -> 보통 False로 지정\n","#                   input_shape=(150, 150, 3),\n","#                   )\n","conv_base = ResNet50V2(weights='imagenet',\n","                       include_top=False,\n","                       input_shape=(150, 150, 3),\n","                       )"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94674944/94668760 [==============================] - 1s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2_V3a27JOxxV","executionInfo":{"status":"ok","timestamp":1619599462081,"user_tz":-540,"elapsed":973,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"0571e60f-a639-4a02-f437-eec72ffce904"},"source":["conv_base.summary()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Model: \"resnet50v2\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            [(None, 150, 150, 3) 0                                            \n","__________________________________________________________________________________________________\n","conv1_pad (ZeroPadding2D)       (None, 156, 156, 3)  0           input_2[0][0]                    \n","__________________________________________________________________________________________________\n","conv1_conv (Conv2D)             (None, 75, 75, 64)   9472        conv1_pad[0][0]                  \n","__________________________________________________________________________________________________\n","pool1_pad (ZeroPadding2D)       (None, 77, 77, 64)   0           conv1_conv[0][0]                 \n","__________________________________________________________________________________________________\n","pool1_pool (MaxPooling2D)       (None, 38, 38, 64)   0           pool1_pad[0][0]                  \n","__________________________________________________________________________________________________\n","conv2_block1_preact_bn (BatchNo (None, 38, 38, 64)   256         pool1_pool[0][0]                 \n","__________________________________________________________________________________________________\n","conv2_block1_preact_relu (Activ (None, 38, 38, 64)   0           conv2_block1_preact_bn[0][0]     \n","__________________________________________________________________________________________________\n","conv2_block1_1_conv (Conv2D)    (None, 38, 38, 64)   4096        conv2_block1_preact_relu[0][0]   \n","__________________________________________________________________________________________________\n","conv2_block1_1_bn (BatchNormali (None, 38, 38, 64)   256         conv2_block1_1_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv2_block1_1_relu (Activation (None, 38, 38, 64)   0           conv2_block1_1_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv2_block1_2_pad (ZeroPadding (None, 40, 40, 64)   0           conv2_block1_1_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv2_block1_2_conv (Conv2D)    (None, 38, 38, 64)   36864       conv2_block1_2_pad[0][0]         \n","__________________________________________________________________________________________________\n","conv2_block1_2_bn (BatchNormali (None, 38, 38, 64)   256         conv2_block1_2_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv2_block1_2_relu (Activation (None, 38, 38, 64)   0           conv2_block1_2_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv2_block1_0_conv (Conv2D)    (None, 38, 38, 256)  16640       conv2_block1_preact_relu[0][0]   \n","__________________________________________________________________________________________________\n","conv2_block1_3_conv (Conv2D)    (None, 38, 38, 256)  16640       conv2_block1_2_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv2_block1_out (Add)          (None, 38, 38, 256)  0           conv2_block1_0_conv[0][0]        \n","                                                                 conv2_block1_3_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv2_block2_preact_bn (BatchNo (None, 38, 38, 256)  1024        conv2_block1_out[0][0]           \n","__________________________________________________________________________________________________\n","conv2_block2_preact_relu (Activ (None, 38, 38, 256)  0           conv2_block2_preact_bn[0][0]     \n","__________________________________________________________________________________________________\n","conv2_block2_1_conv (Conv2D)    (None, 38, 38, 64)   16384       conv2_block2_preact_relu[0][0]   \n","__________________________________________________________________________________________________\n","conv2_block2_1_bn (BatchNormali (None, 38, 38, 64)   256         conv2_block2_1_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv2_block2_1_relu (Activation (None, 38, 38, 64)   0           conv2_block2_1_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv2_block2_2_pad (ZeroPadding (None, 40, 40, 64)   0           conv2_block2_1_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv2_block2_2_conv (Conv2D)    (None, 38, 38, 64)   36864       conv2_block2_2_pad[0][0]         \n","__________________________________________________________________________________________________\n","conv2_block2_2_bn (BatchNormali (None, 38, 38, 64)   256         conv2_block2_2_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv2_block2_2_relu (Activation (None, 38, 38, 64)   0           conv2_block2_2_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv2_block2_3_conv (Conv2D)    (None, 38, 38, 256)  16640       conv2_block2_2_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv2_block2_out (Add)          (None, 38, 38, 256)  0           conv2_block1_out[0][0]           \n","                                                                 conv2_block2_3_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv2_block3_preact_bn (BatchNo (None, 38, 38, 256)  1024        conv2_block2_out[0][0]           \n","__________________________________________________________________________________________________\n","conv2_block3_preact_relu (Activ (None, 38, 38, 256)  0           conv2_block3_preact_bn[0][0]     \n","__________________________________________________________________________________________________\n","conv2_block3_1_conv (Conv2D)    (None, 38, 38, 64)   16384       conv2_block3_preact_relu[0][0]   \n","__________________________________________________________________________________________________\n","conv2_block3_1_bn (BatchNormali (None, 38, 38, 64)   256         conv2_block3_1_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv2_block3_1_relu (Activation (None, 38, 38, 64)   0           conv2_block3_1_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv2_block3_2_pad (ZeroPadding (None, 40, 40, 64)   0           conv2_block3_1_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv2_block3_2_conv (Conv2D)    (None, 19, 19, 64)   36864       conv2_block3_2_pad[0][0]         \n","__________________________________________________________________________________________________\n","conv2_block3_2_bn (BatchNormali (None, 19, 19, 64)   256         conv2_block3_2_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv2_block3_2_relu (Activation (None, 19, 19, 64)   0           conv2_block3_2_bn[0][0]          \n","__________________________________________________________________________________________________\n","max_pooling2d (MaxPooling2D)    (None, 19, 19, 256)  0           conv2_block2_out[0][0]           \n","__________________________________________________________________________________________________\n","conv2_block3_3_conv (Conv2D)    (None, 19, 19, 256)  16640       conv2_block3_2_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv2_block3_out (Add)          (None, 19, 19, 256)  0           max_pooling2d[0][0]              \n","                                                                 conv2_block3_3_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv3_block1_preact_bn (BatchNo (None, 19, 19, 256)  1024        conv2_block3_out[0][0]           \n","__________________________________________________________________________________________________\n","conv3_block1_preact_relu (Activ (None, 19, 19, 256)  0           conv3_block1_preact_bn[0][0]     \n","__________________________________________________________________________________________________\n","conv3_block1_1_conv (Conv2D)    (None, 19, 19, 128)  32768       conv3_block1_preact_relu[0][0]   \n","__________________________________________________________________________________________________\n","conv3_block1_1_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block1_1_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv3_block1_1_relu (Activation (None, 19, 19, 128)  0           conv3_block1_1_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv3_block1_2_pad (ZeroPadding (None, 21, 21, 128)  0           conv3_block1_1_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv3_block1_2_conv (Conv2D)    (None, 19, 19, 128)  147456      conv3_block1_2_pad[0][0]         \n","__________________________________________________________________________________________________\n","conv3_block1_2_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block1_2_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv3_block1_2_relu (Activation (None, 19, 19, 128)  0           conv3_block1_2_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv3_block1_0_conv (Conv2D)    (None, 19, 19, 512)  131584      conv3_block1_preact_relu[0][0]   \n","__________________________________________________________________________________________________\n","conv3_block1_3_conv (Conv2D)    (None, 19, 19, 512)  66048       conv3_block1_2_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv3_block1_out (Add)          (None, 19, 19, 512)  0           conv3_block1_0_conv[0][0]        \n","                                                                 conv3_block1_3_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv3_block2_preact_bn (BatchNo (None, 19, 19, 512)  2048        conv3_block1_out[0][0]           \n","__________________________________________________________________________________________________\n","conv3_block2_preact_relu (Activ (None, 19, 19, 512)  0           conv3_block2_preact_bn[0][0]     \n","__________________________________________________________________________________________________\n","conv3_block2_1_conv (Conv2D)    (None, 19, 19, 128)  65536       conv3_block2_preact_relu[0][0]   \n","__________________________________________________________________________________________________\n","conv3_block2_1_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block2_1_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv3_block2_1_relu (Activation (None, 19, 19, 128)  0           conv3_block2_1_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv3_block2_2_pad (ZeroPadding (None, 21, 21, 128)  0           conv3_block2_1_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv3_block2_2_conv (Conv2D)    (None, 19, 19, 128)  147456      conv3_block2_2_pad[0][0]         \n","__________________________________________________________________________________________________\n","conv3_block2_2_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block2_2_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv3_block2_2_relu (Activation (None, 19, 19, 128)  0           conv3_block2_2_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv3_block2_3_conv (Conv2D)    (None, 19, 19, 512)  66048       conv3_block2_2_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv3_block2_out (Add)          (None, 19, 19, 512)  0           conv3_block1_out[0][0]           \n","                                                                 conv3_block2_3_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv3_block3_preact_bn (BatchNo (None, 19, 19, 512)  2048        conv3_block2_out[0][0]           \n","__________________________________________________________________________________________________\n","conv3_block3_preact_relu (Activ (None, 19, 19, 512)  0           conv3_block3_preact_bn[0][0]     \n","__________________________________________________________________________________________________\n","conv3_block3_1_conv (Conv2D)    (None, 19, 19, 128)  65536       conv3_block3_preact_relu[0][0]   \n","__________________________________________________________________________________________________\n","conv3_block3_1_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block3_1_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv3_block3_1_relu (Activation (None, 19, 19, 128)  0           conv3_block3_1_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv3_block3_2_pad (ZeroPadding (None, 21, 21, 128)  0           conv3_block3_1_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv3_block3_2_conv (Conv2D)    (None, 19, 19, 128)  147456      conv3_block3_2_pad[0][0]         \n","__________________________________________________________________________________________________\n","conv3_block3_2_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block3_2_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv3_block3_2_relu (Activation (None, 19, 19, 128)  0           conv3_block3_2_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv3_block3_3_conv (Conv2D)    (None, 19, 19, 512)  66048       conv3_block3_2_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv3_block3_out (Add)          (None, 19, 19, 512)  0           conv3_block2_out[0][0]           \n","                                                                 conv3_block3_3_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv3_block4_preact_bn (BatchNo (None, 19, 19, 512)  2048        conv3_block3_out[0][0]           \n","__________________________________________________________________________________________________\n","conv3_block4_preact_relu (Activ (None, 19, 19, 512)  0           conv3_block4_preact_bn[0][0]     \n","__________________________________________________________________________________________________\n","conv3_block4_1_conv (Conv2D)    (None, 19, 19, 128)  65536       conv3_block4_preact_relu[0][0]   \n","__________________________________________________________________________________________________\n","conv3_block4_1_bn (BatchNormali (None, 19, 19, 128)  512         conv3_block4_1_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv3_block4_1_relu (Activation (None, 19, 19, 128)  0           conv3_block4_1_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv3_block4_2_pad (ZeroPadding (None, 21, 21, 128)  0           conv3_block4_1_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv3_block4_2_conv (Conv2D)    (None, 10, 10, 128)  147456      conv3_block4_2_pad[0][0]         \n","__________________________________________________________________________________________________\n","conv3_block4_2_bn (BatchNormali (None, 10, 10, 128)  512         conv3_block4_2_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv3_block4_2_relu (Activation (None, 10, 10, 128)  0           conv3_block4_2_bn[0][0]          \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 10, 10, 512)  0           conv3_block3_out[0][0]           \n","__________________________________________________________________________________________________\n","conv3_block4_3_conv (Conv2D)    (None, 10, 10, 512)  66048       conv3_block4_2_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv3_block4_out (Add)          (None, 10, 10, 512)  0           max_pooling2d_1[0][0]            \n","                                                                 conv3_block4_3_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block1_preact_bn (BatchNo (None, 10, 10, 512)  2048        conv3_block4_out[0][0]           \n","__________________________________________________________________________________________________\n","conv4_block1_preact_relu (Activ (None, 10, 10, 512)  0           conv4_block1_preact_bn[0][0]     \n","__________________________________________________________________________________________________\n","conv4_block1_1_conv (Conv2D)    (None, 10, 10, 256)  131072      conv4_block1_preact_relu[0][0]   \n","__________________________________________________________________________________________________\n","conv4_block1_1_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block1_1_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block1_1_relu (Activation (None, 10, 10, 256)  0           conv4_block1_1_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv4_block1_2_pad (ZeroPadding (None, 12, 12, 256)  0           conv4_block1_1_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block1_2_conv (Conv2D)    (None, 10, 10, 256)  589824      conv4_block1_2_pad[0][0]         \n","__________________________________________________________________________________________________\n","conv4_block1_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block1_2_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block1_2_relu (Activation (None, 10, 10, 256)  0           conv4_block1_2_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv4_block1_0_conv (Conv2D)    (None, 10, 10, 1024) 525312      conv4_block1_preact_relu[0][0]   \n","__________________________________________________________________________________________________\n","conv4_block1_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block1_2_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block1_out (Add)          (None, 10, 10, 1024) 0           conv4_block1_0_conv[0][0]        \n","                                                                 conv4_block1_3_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block2_preact_bn (BatchNo (None, 10, 10, 1024) 4096        conv4_block1_out[0][0]           \n","__________________________________________________________________________________________________\n","conv4_block2_preact_relu (Activ (None, 10, 10, 1024) 0           conv4_block2_preact_bn[0][0]     \n","__________________________________________________________________________________________________\n","conv4_block2_1_conv (Conv2D)    (None, 10, 10, 256)  262144      conv4_block2_preact_relu[0][0]   \n","__________________________________________________________________________________________________\n","conv4_block2_1_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block2_1_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block2_1_relu (Activation (None, 10, 10, 256)  0           conv4_block2_1_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv4_block2_2_pad (ZeroPadding (None, 12, 12, 256)  0           conv4_block2_1_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block2_2_conv (Conv2D)    (None, 10, 10, 256)  589824      conv4_block2_2_pad[0][0]         \n","__________________________________________________________________________________________________\n","conv4_block2_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block2_2_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block2_2_relu (Activation (None, 10, 10, 256)  0           conv4_block2_2_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv4_block2_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block2_2_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block2_out (Add)          (None, 10, 10, 1024) 0           conv4_block1_out[0][0]           \n","                                                                 conv4_block2_3_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block3_preact_bn (BatchNo (None, 10, 10, 1024) 4096        conv4_block2_out[0][0]           \n","__________________________________________________________________________________________________\n","conv4_block3_preact_relu (Activ (None, 10, 10, 1024) 0           conv4_block3_preact_bn[0][0]     \n","__________________________________________________________________________________________________\n","conv4_block3_1_conv (Conv2D)    (None, 10, 10, 256)  262144      conv4_block3_preact_relu[0][0]   \n","__________________________________________________________________________________________________\n","conv4_block3_1_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block3_1_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block3_1_relu (Activation (None, 10, 10, 256)  0           conv4_block3_1_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv4_block3_2_pad (ZeroPadding (None, 12, 12, 256)  0           conv4_block3_1_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block3_2_conv (Conv2D)    (None, 10, 10, 256)  589824      conv4_block3_2_pad[0][0]         \n","__________________________________________________________________________________________________\n","conv4_block3_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block3_2_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block3_2_relu (Activation (None, 10, 10, 256)  0           conv4_block3_2_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv4_block3_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block3_2_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block3_out (Add)          (None, 10, 10, 1024) 0           conv4_block2_out[0][0]           \n","                                                                 conv4_block3_3_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block4_preact_bn (BatchNo (None, 10, 10, 1024) 4096        conv4_block3_out[0][0]           \n","__________________________________________________________________________________________________\n","conv4_block4_preact_relu (Activ (None, 10, 10, 1024) 0           conv4_block4_preact_bn[0][0]     \n","__________________________________________________________________________________________________\n","conv4_block4_1_conv (Conv2D)    (None, 10, 10, 256)  262144      conv4_block4_preact_relu[0][0]   \n","__________________________________________________________________________________________________\n","conv4_block4_1_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block4_1_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block4_1_relu (Activation (None, 10, 10, 256)  0           conv4_block4_1_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv4_block4_2_pad (ZeroPadding (None, 12, 12, 256)  0           conv4_block4_1_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block4_2_conv (Conv2D)    (None, 10, 10, 256)  589824      conv4_block4_2_pad[0][0]         \n","__________________________________________________________________________________________________\n","conv4_block4_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block4_2_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block4_2_relu (Activation (None, 10, 10, 256)  0           conv4_block4_2_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv4_block4_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block4_2_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block4_out (Add)          (None, 10, 10, 1024) 0           conv4_block3_out[0][0]           \n","                                                                 conv4_block4_3_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block5_preact_bn (BatchNo (None, 10, 10, 1024) 4096        conv4_block4_out[0][0]           \n","__________________________________________________________________________________________________\n","conv4_block5_preact_relu (Activ (None, 10, 10, 1024) 0           conv4_block5_preact_bn[0][0]     \n","__________________________________________________________________________________________________\n","conv4_block5_1_conv (Conv2D)    (None, 10, 10, 256)  262144      conv4_block5_preact_relu[0][0]   \n","__________________________________________________________________________________________________\n","conv4_block5_1_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block5_1_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block5_1_relu (Activation (None, 10, 10, 256)  0           conv4_block5_1_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv4_block5_2_pad (ZeroPadding (None, 12, 12, 256)  0           conv4_block5_1_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block5_2_conv (Conv2D)    (None, 10, 10, 256)  589824      conv4_block5_2_pad[0][0]         \n","__________________________________________________________________________________________________\n","conv4_block5_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block5_2_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block5_2_relu (Activation (None, 10, 10, 256)  0           conv4_block5_2_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv4_block5_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block5_2_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block5_out (Add)          (None, 10, 10, 1024) 0           conv4_block4_out[0][0]           \n","                                                                 conv4_block5_3_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block6_preact_bn (BatchNo (None, 10, 10, 1024) 4096        conv4_block5_out[0][0]           \n","__________________________________________________________________________________________________\n","conv4_block6_preact_relu (Activ (None, 10, 10, 1024) 0           conv4_block6_preact_bn[0][0]     \n","__________________________________________________________________________________________________\n","conv4_block6_1_conv (Conv2D)    (None, 10, 10, 256)  262144      conv4_block6_preact_relu[0][0]   \n","__________________________________________________________________________________________________\n","conv4_block6_1_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block6_1_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block6_1_relu (Activation (None, 10, 10, 256)  0           conv4_block6_1_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv4_block6_2_pad (ZeroPadding (None, 12, 12, 256)  0           conv4_block6_1_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block6_2_conv (Conv2D)    (None, 5, 5, 256)    589824      conv4_block6_2_pad[0][0]         \n","__________________________________________________________________________________________________\n","conv4_block6_2_bn (BatchNormali (None, 5, 5, 256)    1024        conv4_block6_2_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block6_2_relu (Activation (None, 5, 5, 256)    0           conv4_block6_2_bn[0][0]          \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 5, 5, 1024)   0           conv4_block5_out[0][0]           \n","__________________________________________________________________________________________________\n","conv4_block6_3_conv (Conv2D)    (None, 5, 5, 1024)   263168      conv4_block6_2_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv4_block6_out (Add)          (None, 5, 5, 1024)   0           max_pooling2d_2[0][0]            \n","                                                                 conv4_block6_3_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv5_block1_preact_bn (BatchNo (None, 5, 5, 1024)   4096        conv4_block6_out[0][0]           \n","__________________________________________________________________________________________________\n","conv5_block1_preact_relu (Activ (None, 5, 5, 1024)   0           conv5_block1_preact_bn[0][0]     \n","__________________________________________________________________________________________________\n","conv5_block1_1_conv (Conv2D)    (None, 5, 5, 512)    524288      conv5_block1_preact_relu[0][0]   \n","__________________________________________________________________________________________________\n","conv5_block1_1_bn (BatchNormali (None, 5, 5, 512)    2048        conv5_block1_1_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv5_block1_1_relu (Activation (None, 5, 5, 512)    0           conv5_block1_1_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv5_block1_2_pad (ZeroPadding (None, 7, 7, 512)    0           conv5_block1_1_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv5_block1_2_conv (Conv2D)    (None, 5, 5, 512)    2359296     conv5_block1_2_pad[0][0]         \n","__________________________________________________________________________________________________\n","conv5_block1_2_bn (BatchNormali (None, 5, 5, 512)    2048        conv5_block1_2_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv5_block1_2_relu (Activation (None, 5, 5, 512)    0           conv5_block1_2_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv5_block1_0_conv (Conv2D)    (None, 5, 5, 2048)   2099200     conv5_block1_preact_relu[0][0]   \n","__________________________________________________________________________________________________\n","conv5_block1_3_conv (Conv2D)    (None, 5, 5, 2048)   1050624     conv5_block1_2_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv5_block1_out (Add)          (None, 5, 5, 2048)   0           conv5_block1_0_conv[0][0]        \n","                                                                 conv5_block1_3_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv5_block2_preact_bn (BatchNo (None, 5, 5, 2048)   8192        conv5_block1_out[0][0]           \n","__________________________________________________________________________________________________\n","conv5_block2_preact_relu (Activ (None, 5, 5, 2048)   0           conv5_block2_preact_bn[0][0]     \n","__________________________________________________________________________________________________\n","conv5_block2_1_conv (Conv2D)    (None, 5, 5, 512)    1048576     conv5_block2_preact_relu[0][0]   \n","__________________________________________________________________________________________________\n","conv5_block2_1_bn (BatchNormali (None, 5, 5, 512)    2048        conv5_block2_1_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv5_block2_1_relu (Activation (None, 5, 5, 512)    0           conv5_block2_1_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv5_block2_2_pad (ZeroPadding (None, 7, 7, 512)    0           conv5_block2_1_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv5_block2_2_conv (Conv2D)    (None, 5, 5, 512)    2359296     conv5_block2_2_pad[0][0]         \n","__________________________________________________________________________________________________\n","conv5_block2_2_bn (BatchNormali (None, 5, 5, 512)    2048        conv5_block2_2_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv5_block2_2_relu (Activation (None, 5, 5, 512)    0           conv5_block2_2_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv5_block2_3_conv (Conv2D)    (None, 5, 5, 2048)   1050624     conv5_block2_2_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv5_block2_out (Add)          (None, 5, 5, 2048)   0           conv5_block1_out[0][0]           \n","                                                                 conv5_block2_3_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv5_block3_preact_bn (BatchNo (None, 5, 5, 2048)   8192        conv5_block2_out[0][0]           \n","__________________________________________________________________________________________________\n","conv5_block3_preact_relu (Activ (None, 5, 5, 2048)   0           conv5_block3_preact_bn[0][0]     \n","__________________________________________________________________________________________________\n","conv5_block3_1_conv (Conv2D)    (None, 5, 5, 512)    1048576     conv5_block3_preact_relu[0][0]   \n","__________________________________________________________________________________________________\n","conv5_block3_1_bn (BatchNormali (None, 5, 5, 512)    2048        conv5_block3_1_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv5_block3_1_relu (Activation (None, 5, 5, 512)    0           conv5_block3_1_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv5_block3_2_pad (ZeroPadding (None, 7, 7, 512)    0           conv5_block3_1_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv5_block3_2_conv (Conv2D)    (None, 5, 5, 512)    2359296     conv5_block3_2_pad[0][0]         \n","__________________________________________________________________________________________________\n","conv5_block3_2_bn (BatchNormali (None, 5, 5, 512)    2048        conv5_block3_2_conv[0][0]        \n","__________________________________________________________________________________________________\n","conv5_block3_2_relu (Activation (None, 5, 5, 512)    0           conv5_block3_2_bn[0][0]          \n","__________________________________________________________________________________________________\n","conv5_block3_3_conv (Conv2D)    (None, 5, 5, 2048)   1050624     conv5_block3_2_relu[0][0]        \n","__________________________________________________________________________________________________\n","conv5_block3_out (Add)          (None, 5, 5, 2048)   0           conv5_block2_out[0][0]           \n","                                                                 conv5_block3_3_conv[0][0]        \n","__________________________________________________________________________________________________\n","post_bn (BatchNormalization)    (None, 5, 5, 2048)   8192        conv5_block3_out[0][0]           \n","__________________________________________________________________________________________________\n","post_relu (Activation)          (None, 5, 5, 2048)   0           post_bn[0][0]                    \n","==================================================================================================\n","Total params: 23,564,800\n","Trainable params: 23,519,360\n","Non-trainable params: 45,440\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sSLIxrNJOxsL"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HQuEWKntNt0P"},"source":["## Feature extraction의 두 가지 방법\n","1. **빠른 추출방식**\n","    - 예측하려는 새로운 데이터를 위의 `conv_base`에 입력하여 나온 출력값을 numpy 배열로 저장하고 이를 분류 모델의 입력값으로 사용. Convolution operation을 하지 않아도 되기 때문에 빠르게 학습. 하지만 data augmentation 방법을 사용할 수 없음.\n","\n","2. **받아온 특성 Layer를 이용해 새로운 모델 구현하는 방식**\n","    - 위의 `conv_base` 이후에 새로운 layer를 쌓아 확장한 뒤 전체 모델을 다시 학습. 모든 데이터가 convolution layer들을 통과해야 하기 때문에 학습이 느림. 단 conv_base의 가중치는 업데이트 되지 않도록 한다. data augmentation 방법을 사용할 수 있음."]},{"cell_type":"markdown","metadata":{"id":"1MAxTeKpNt0P"},"source":["### 빠른 특성 추출 방식\n"]},{"cell_type":"markdown","metadata":{"id":"TF3Z1_jINt0P"},"source":["- `conv_base`의 predict 메소드로 입력 이미지의 feature를 추출 "]},{"cell_type":"code","metadata":{"id":"GXKwzNokNt0P","executionInfo":{"status":"ok","timestamp":1619599783892,"user_tz":-540,"elapsed":762,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}}},"source":["# 하이퍼파라미터\n","LEARNING_RATE = 0.001\n","N_EPOCHS = 30\n","N_BATCHS = 100"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"XD-sejzUNt0P","executionInfo":{"status":"ok","timestamp":1619599786066,"user_tz":-540,"elapsed":823,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.applications import VGG16, ResNet50V2\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","import numpy as np\n","\n","np.random.seed(1)\n","tf.random.set_seed(1)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"YvDR_5nzNt0Q","executionInfo":{"status":"ok","timestamp":1619577357709,"user_tz":-540,"elapsed":778,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}}},"source":["def next_featuremap(image_directory, sample_counts):\n","    \"\"\"\n","    매개변수로 받은 디렉토리의 이미지들을 Conv_base(VGG16) 모델을 통과시켜서 Featuremap들을 추출해 반환하는 함수\n","    [매개변수]\n","        image_directory: 이미지 데이터들이 있는 디렉토리\n","        sample_counts: 특성을 추출할 이미지 개수\n","    [반환값]\n","        tuple: (featuremap들, label)\n","    \"\"\"\n","    conv_base = VGG16(weights='imagenet',\n","                      include_top=False,\n","                      input_shape=(150, 150, 3),\n","                      )\n","    # 결과를 담을 ndarray\n","    # featuremap 저장, conv_base의 마지막 layer의 output의 shape에 맞춘다.\n","    return_features = np.zeros(shape=(sample_counts, 4, 4, 512))  # 위에서 150,150,3의 출력이 4,4,512였기 때문에\n","    return_labels = np.zeros(shape=(sample_counts, ))  # label들을 저장\n","\n","    datagen = ImageDataGenerator(rescale=1./255)\n","    iterator = datagen.flow_from_directory(image_directory,\n","                                           target_size=(150, 150),\n","                                           batch_size=N_BATCHS,\n","                                           class_mode='binary',\n","                                           )\n","    \n","    i = 0  # 반복횟수를 저장할 변수\n","    for input_batch, label_batch in iterator:  # (image, label)*batch크기(100) 만큼 튜플로 반환\n","        # input_batch를 conv_base에 넣어서 feature map을 추출 - model.predict(): 모델의 레이어들을 통과해서 나온 출력결과를 반환\n","        fm = conv_base.predict(input_batch)\n","\n","        return_features[i*N_BATCHS: (i+1)*N_BATCHS] = fm\n","        return_labels[i*N_BATCHS: (i+1)*N_BATCHS] = label_batch\n","        i += 1\n","        if i*N_BATCHS >= sample_counts:  # 결과를 저장할 배열의 시작 index가 sample_counts보다 크면 반복을 멈춘다.\n","            break\n","    \n","    return return_features, return_labels\n"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"47KhspvApuBK","executionInfo":{"status":"ok","timestamp":1619599790173,"user_tz":-540,"elapsed":756,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}}},"source":["def next_featuremap(image_dir, sample_counts):\n","    conv_base = ResNet50V2(weights='imagenet',\n","                           include_top=False,\n","                           input_shape=(150, 150, 3),\n","                           )\n","    return_features = np.zeros(shape=(sample_counts, 5, 5, 2048))\n","    return_labels = np.zeros(shape=(sample_counts, ))\n","\n","    datagen = ImageDataGenerator(rescale=1/255.)\n","    iterator = datagen.flow_from_directory(image_dir,\n","                                           target_size=(150, 150),\n","                                           batch_size=N_BATCHS,\n","                                           class_mode='binary'\n","                                           )\n","    i = 0\n","    for input_batch, label_batch in iterator:\n","        fm = conv_base.predict(input_batch)\n","\n","        return_features[i*N_BATCHS: (i+1)*N_BATCHS] = fm\n","        return_labels[i*N_BATCHS: (i+1)*N_BATCHS] = label_batch\n","        i += 1\n","        if i*N_BATCHS >= sample_counts:\n","            break\n","    return return_features, return_labels"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"BAzLBQ5JNt0Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619599849759,"user_tz":-540,"elapsed":58689,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"0fc21674-6906-4ffb-e241-2bfa7d633212"},"source":["train_dir = '/content/data/cats_and_dogs_small/train'\n","validation_dir = '/content/data/cats_and_dogs_small/validation'\n","test_dir = '/content/data/cats_and_dogs_small/test'\n","\n","# Featuremap 추출\n","train_features, train_labels = next_featuremap(train_dir, 2000)\n","validation_features, validation_labels = next_featuremap(validation_dir, 1000)\n","test_featues, test_labels = next_featuremap(test_dir, 1000)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Found 2000 images belonging to 2 classes.\n","Found 1000 images belonging to 2 classes.\n","Found 1000 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"01tH9xcONt0Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619599853960,"user_tz":-540,"elapsed":998,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"0309319d-fb5a-4d77-8cd5-726b84a09c12"},"source":["train_features.shape"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2000, 5, 5, 2048)"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"OEjPGW42Nt0Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619599855805,"user_tz":-540,"elapsed":795,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"7f73039b-18fb-42f1-fca5-4bb0102af0e5"},"source":["train_labels.shape"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2000,)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"A_UieHHMNt0Q","executionInfo":{"status":"ok","timestamp":1619600011865,"user_tz":-540,"elapsed":810,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}}},"source":["def create_model():\n","    # 분류기 모델만 생성\n","    model = keras.Sequential()\n","    # model.add(layers.Input(shape=(4, 4, 512)))\n","    model.add(layers.Input(shape=(5, 5, 2048)))\n","    model.add(layers.GlobalAveragePooling2D())\n","    model.add(layers.Dense(256, activation='relu'))\n","    model.add(layers.Dense(1, activation='sigmoid'))\n","\n","    return model"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"eb23MWOuNt0Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619600013104,"user_tz":-540,"elapsed":720,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"4cb80b62-a832-4bab-ad0b-e9093041d97f"},"source":["model = create_model()\n","model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n","              loss='binary_crossentropy',\n","              metrics=['accuracy']\n","              )\n","model.summary()"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","global_average_pooling2d_1 ( (None, 2048)              0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 256)               524544    \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 1)                 257       \n","=================================================================\n","Total params: 524,801\n","Trainable params: 524,801\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XBvZBNtnWwVY","executionInfo":{"status":"ok","timestamp":1619600025806,"user_tz":-540,"elapsed":12091,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"2b80af7e-ab00-4d6d-bced-b7fccce08477"},"source":["# N_EPOCHS = 100\n","history = model.fit(train_features, train_labels,\n","                    epochs=N_EPOCHS,\n","                    batch_size=N_BATCHS,\n","                    validation_data=(validation_features,validation_labels),\n","                    )"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n","20/20 [==============================] - 1s 29ms/step - loss: 0.4370 - accuracy: 0.8397 - val_loss: 0.1551 - val_accuracy: 0.9470\n","Epoch 2/30\n","20/20 [==============================] - 0s 17ms/step - loss: 0.1555 - accuracy: 0.9579 - val_loss: 0.1127 - val_accuracy: 0.9550\n","Epoch 3/30\n","20/20 [==============================] - 0s 17ms/step - loss: 0.0646 - accuracy: 0.9773 - val_loss: 0.1037 - val_accuracy: 0.9590\n","Epoch 4/30\n","20/20 [==============================] - 0s 17ms/step - loss: 0.0304 - accuracy: 0.9934 - val_loss: 0.1112 - val_accuracy: 0.9580\n","Epoch 5/30\n","20/20 [==============================] - 0s 17ms/step - loss: 0.0186 - accuracy: 0.9983 - val_loss: 0.1131 - val_accuracy: 0.9600\n","Epoch 6/30\n","20/20 [==============================] - 0s 18ms/step - loss: 0.0149 - accuracy: 0.9971 - val_loss: 0.1229 - val_accuracy: 0.9600\n","Epoch 7/30\n","20/20 [==============================] - 0s 17ms/step - loss: 0.0070 - accuracy: 0.9998 - val_loss: 0.1167 - val_accuracy: 0.9620\n","Epoch 8/30\n","20/20 [==============================] - 0s 18ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.1234 - val_accuracy: 0.9620\n","Epoch 9/30\n","20/20 [==============================] - 0s 17ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.1255 - val_accuracy: 0.9640\n","Epoch 10/30\n","20/20 [==============================] - 0s 17ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.1265 - val_accuracy: 0.9660\n","Epoch 11/30\n","20/20 [==============================] - 0s 17ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.1294 - val_accuracy: 0.9640\n","Epoch 12/30\n","20/20 [==============================] - 0s 18ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1282 - val_accuracy: 0.9640\n","Epoch 13/30\n","20/20 [==============================] - 0s 17ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1326 - val_accuracy: 0.9640\n","Epoch 14/30\n","20/20 [==============================] - 0s 18ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.1348 - val_accuracy: 0.9640\n","Epoch 15/30\n","20/20 [==============================] - 0s 17ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1355 - val_accuracy: 0.9640\n","Epoch 16/30\n","20/20 [==============================] - 0s 17ms/step - loss: 9.5255e-04 - accuracy: 1.0000 - val_loss: 0.1377 - val_accuracy: 0.9640\n","Epoch 17/30\n","20/20 [==============================] - 0s 18ms/step - loss: 9.2030e-04 - accuracy: 1.0000 - val_loss: 0.1367 - val_accuracy: 0.9620\n","Epoch 18/30\n","20/20 [==============================] - 0s 18ms/step - loss: 7.9748e-04 - accuracy: 1.0000 - val_loss: 0.1391 - val_accuracy: 0.9640\n","Epoch 19/30\n","20/20 [==============================] - 0s 17ms/step - loss: 6.7347e-04 - accuracy: 1.0000 - val_loss: 0.1402 - val_accuracy: 0.9640\n","Epoch 20/30\n","20/20 [==============================] - 0s 18ms/step - loss: 6.4245e-04 - accuracy: 1.0000 - val_loss: 0.1425 - val_accuracy: 0.9640\n","Epoch 21/30\n","20/20 [==============================] - 0s 17ms/step - loss: 5.4680e-04 - accuracy: 1.0000 - val_loss: 0.1438 - val_accuracy: 0.9630\n","Epoch 22/30\n","20/20 [==============================] - 0s 17ms/step - loss: 5.0656e-04 - accuracy: 1.0000 - val_loss: 0.1443 - val_accuracy: 0.9630\n","Epoch 23/30\n","20/20 [==============================] - 0s 17ms/step - loss: 4.7499e-04 - accuracy: 1.0000 - val_loss: 0.1468 - val_accuracy: 0.9620\n","Epoch 24/30\n","20/20 [==============================] - 0s 18ms/step - loss: 4.3393e-04 - accuracy: 1.0000 - val_loss: 0.1475 - val_accuracy: 0.9630\n","Epoch 25/30\n","20/20 [==============================] - 0s 17ms/step - loss: 3.6581e-04 - accuracy: 1.0000 - val_loss: 0.1507 - val_accuracy: 0.9640\n","Epoch 26/30\n","20/20 [==============================] - 0s 17ms/step - loss: 3.3243e-04 - accuracy: 1.0000 - val_loss: 0.1498 - val_accuracy: 0.9610\n","Epoch 27/30\n","20/20 [==============================] - 0s 18ms/step - loss: 2.9650e-04 - accuracy: 1.0000 - val_loss: 0.1506 - val_accuracy: 0.9620\n","Epoch 28/30\n","20/20 [==============================] - 0s 18ms/step - loss: 3.0835e-04 - accuracy: 1.0000 - val_loss: 0.1536 - val_accuracy: 0.9620\n","Epoch 29/30\n","20/20 [==============================] - 0s 18ms/step - loss: 2.8464e-04 - accuracy: 1.0000 - val_loss: 0.1532 - val_accuracy: 0.9610\n","Epoch 30/30\n","20/20 [==============================] - 0s 17ms/step - loss: 2.4242e-04 - accuracy: 1.0000 - val_loss: 0.1548 - val_accuracy: 0.9610\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9-qt78b8WwS3","executionInfo":{"status":"ok","timestamp":1619600113088,"user_tz":-540,"elapsed":924,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}}},"source":["from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","\n","# 한 개 이미지 추론\n","def predict_cat_dog(path, model, mode=False):\n","    class_name = ['cat', 'dog']\n","    img = load_img(path, target_size=(150, 150, 3))\n","    sample = img_to_array(img)[np.newaxis, ...]\n","    sample = sample/255.\n","    if mode:  # conv_base를 거치도록\n","        cb = ResNet50V2(weights='imagenet',\n","                        include_top=False,\n","                        input_shape=(150, 150, 3),\n","                        )\n","        sample = cb.predict(sample)\n","\n","    pred = model.predict(sample)\n","    pred_class = np.where(pred < 0.5, 0, 1)\n","    pred_class_name = class_name[pred_class[0,0]]\n","\n","    return pred, pred_class, pred_class_name\n"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jV6IVNGOWwQt","executionInfo":{"status":"ok","timestamp":1619600116304,"user_tz":-540,"elapsed":3723,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"7d70545a-1c53-4ef2-a233-9bacda094419"},"source":["predict_cat_dog('/content/dog.jpg', model, mode=True)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:5 out of the last 44 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fbc00aec560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(array([[0.9999888]], dtype=float32), array([[1]]), 'dog')"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"c2X0Xd3XWwOS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fR7m3cpNNt0Q"},"source":["### Pretrained Network를 이용해 새로운 모델 구현하는 방식"]},{"cell_type":"markdown","metadata":{"id":"Ri-Lq1DUNt0Q"},"source":["- Conv_base의 feature extraction 부분에 fully connected layer를 추가하여 모형 생성 \n","- Conv_base에서 가져온 부분은 학습을 하지 않고 weight를 고정\n","    -  **Layer.trainable=False**"]},{"cell_type":"code","metadata":{"id":"NcRoSfKsNt0R"},"source":["LEARNING_RATE = 0.001\n","N_EPOCHS = 20\n","N_BATCHS = 100\n","IMAGE_SIZE = 150"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oHiFtlydNt0R"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","def create_model():\n","    conv_base = VGG16(weights='imagenet',\n","                      include_top=False,\n","                      input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)\n","                      )\n","    conv_base.trainable = False  # 학습 시 weight 최적화(update)를 하지 않도록 설정 -> 모델 컴파일 전에 실행해야 한다.\n","\n","    model = keras.Sequential()\n","    model.add(conv_base)\n","    model.add(layers.GlobalAveragePooling2D())  # 4*4*512이므로 Flatten 대신 사용 -> 결과: 1*1*512\n","    model.add(layers.Dense(256, activation='relu'))\n","\n","    # 출력\n","    model.add(layers.Dense(1, activation='sigmoid'))\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"17plbkbyNt0R","executionInfo":{"status":"ok","timestamp":1619510266476,"user_tz":-540,"elapsed":1188,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"9313ddad-2128-40b4-eab4-9c906432b46b"},"source":["model = create_model()\n","model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n","              loss='binary_crossentropy',\n","              metrics=['accuracy']\n","              )\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","vgg16 (Functional)           (None, 4, 4, 512)         14714688  \n","_________________________________________________________________\n","global_average_pooling2d (Gl (None, 512)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 256)               131328    \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 257       \n","=================================================================\n","Total params: 14,846,273\n","Trainable params: 131,585\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TSD7R3xGNt0R","executionInfo":{"status":"ok","timestamp":1619510523653,"user_tz":-540,"elapsed":1162,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"d22759f5-3b70-4231-8eda-3d8bd62729aa"},"source":["train_iterator, validation_iterator, test_iterator = get_generators()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 2000 images belonging to 2 classes.\n","Found 1000 images belonging to 2 classes.\n","Found 1000 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cpm0Sx3pWE_0","executionInfo":{"status":"ok","timestamp":1619511043864,"user_tz":-540,"elapsed":428188,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"5905fc6b-f65a-401a-d039-b150c261695f"},"source":["history = model.fit(train_iterator,\n","                    epochs=N_EPOCHS,\n","                    steps_per_epoch=len(train_iterator),\n","                    validation_data=validation_iterator,\n","                    validation_steps=len(validation_iterator)\n","                    )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","20/20 [==============================] - 59s 981ms/step - loss: 0.6436 - accuracy: 0.6259 - val_loss: 0.4333 - val_accuracy: 0.8380\n","Epoch 2/20\n","20/20 [==============================] - 19s 969ms/step - loss: 0.4553 - accuracy: 0.8163 - val_loss: 0.3522 - val_accuracy: 0.8570\n","Epoch 3/20\n","20/20 [==============================] - 19s 960ms/step - loss: 0.3939 - accuracy: 0.8356 - val_loss: 0.3247 - val_accuracy: 0.8770\n","Epoch 4/20\n","20/20 [==============================] - 19s 962ms/step - loss: 0.3646 - accuracy: 0.8360 - val_loss: 0.3495 - val_accuracy: 0.8420\n","Epoch 5/20\n","20/20 [==============================] - 19s 963ms/step - loss: 0.3655 - accuracy: 0.8336 - val_loss: 0.2885 - val_accuracy: 0.8860\n","Epoch 6/20\n","20/20 [==============================] - 19s 967ms/step - loss: 0.3253 - accuracy: 0.8611 - val_loss: 0.2792 - val_accuracy: 0.8840\n","Epoch 7/20\n","20/20 [==============================] - 19s 966ms/step - loss: 0.3317 - accuracy: 0.8431 - val_loss: 0.2703 - val_accuracy: 0.8910\n","Epoch 8/20\n","20/20 [==============================] - 19s 966ms/step - loss: 0.3164 - accuracy: 0.8675 - val_loss: 0.3051 - val_accuracy: 0.8730\n","Epoch 9/20\n","20/20 [==============================] - 19s 970ms/step - loss: 0.3348 - accuracy: 0.8425 - val_loss: 0.2619 - val_accuracy: 0.8960\n","Epoch 10/20\n","20/20 [==============================] - 19s 966ms/step - loss: 0.3065 - accuracy: 0.8580 - val_loss: 0.2751 - val_accuracy: 0.8810\n","Epoch 11/20\n","20/20 [==============================] - 19s 974ms/step - loss: 0.3125 - accuracy: 0.8618 - val_loss: 0.2728 - val_accuracy: 0.8850\n","Epoch 12/20\n","20/20 [==============================] - 19s 970ms/step - loss: 0.3263 - accuracy: 0.8555 - val_loss: 0.2579 - val_accuracy: 0.8910\n","Epoch 13/20\n","20/20 [==============================] - 19s 962ms/step - loss: 0.3075 - accuracy: 0.8622 - val_loss: 0.2572 - val_accuracy: 0.8910\n","Epoch 14/20\n","20/20 [==============================] - 20s 978ms/step - loss: 0.2891 - accuracy: 0.8754 - val_loss: 0.2554 - val_accuracy: 0.8970\n","Epoch 15/20\n","20/20 [==============================] - 19s 970ms/step - loss: 0.2743 - accuracy: 0.8788 - val_loss: 0.2554 - val_accuracy: 0.8980\n","Epoch 16/20\n","20/20 [==============================] - 19s 965ms/step - loss: 0.2836 - accuracy: 0.8812 - val_loss: 0.2827 - val_accuracy: 0.8800\n","Epoch 17/20\n","20/20 [==============================] - 19s 967ms/step - loss: 0.2982 - accuracy: 0.8669 - val_loss: 0.2558 - val_accuracy: 0.8980\n","Epoch 18/20\n","20/20 [==============================] - 19s 973ms/step - loss: 0.2860 - accuracy: 0.8740 - val_loss: 0.2598 - val_accuracy: 0.8940\n","Epoch 19/20\n","20/20 [==============================] - 19s 974ms/step - loss: 0.2706 - accuracy: 0.8900 - val_loss: 0.2519 - val_accuracy: 0.8990\n","Epoch 20/20\n","20/20 [==============================] - 19s 963ms/step - loss: 0.2579 - accuracy: 0.8993 - val_loss: 0.2503 - val_accuracy: 0.8970\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8ywPpayiNt0S"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"veCB8fyQNt0S"},"source":["## 미세조정(Fine-tuning)\n","- Pretrained 모델을 내가 학습시켜야 하는 데이터셋(Custom Dataset)에 재학습시키는 것을 fine tunning 이라고 한다.\n","- 주어진 문제에 더 적합하도록 모델의 가중치들을 조정."]},{"cell_type":"markdown","metadata":{"id":"UAETwhvHNt0S"},"source":["### Fine tuning 전략\n","![image-2.png](attachment:image-2.png)"]},{"cell_type":"markdown","metadata":{"id":"4_25dFKiNt0S"},"source":["- **세 전략 모두 classifier layer들은 train한다.**\n","\n","1. <span style=\"font-size:1.2em;font-weight:bold\">전체 모델을 전부 학습시킨다.(1번)</span>\n","    - Pretrained 모델의 weight는 Feature extraction 의 초기 weight 역할을 한다.\n","    - **Train dataset의 양이 많고** Pretrained 모델이 학습했던 dataset과 Custom dataset의 class간의 유사성이 **낮은 경우** 적용.\n","    - 학습에 시간이 많이 걸린다.\n","2. <span style=\"font-size:1.2em;font-weight:bold\">Pretrained 모델 Bottom layer들(Input과 가까운 Layer들)은 고정시키고 Top layer의 일부를 재학습시킨다.(2번)</span>\n","    - **Train dataset의 양이 많고** Pretrained 모델이 학습했던 dataset과 Custom dataset의 class간의 유사성이 **높은 경우** 적용.\n","    - **Train dataset의 양이 적고** Pretained 모델이 학습했던 dataset과 custom dataset의 class간의 유사성이 **낮은 경우** 적용\n","3. <span style=\"font-size:1.2em;font-weight:bold\">Pretrained 모델 전체를 고정시키고 classifier layer들만 학습시킨다.(3번)</span>\n","    - **Train dataset의 양이 적고** Pretrained 모델이 학습했던 dataset과 Custom dataset의 class간의 유사성이 **높은 경우** 적용.\n","  \n","  \n","> custom dataset: 내가 학습시키고자 하는 dataset "]},{"cell_type":"code","metadata":{"id":"I5qerbngNt0S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619587533368,"user_tz":-540,"elapsed":6812,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"805fe0ab-791e-4314-d9ed-df0a43ee33aa"},"source":["cb = VGG16(include_top=False,\n","           weights='imagenet',\n","           input_shape=(150, 150, 3)\n","           )\n","cb.summary()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 1s 0us/step\n","Model: \"vgg16\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n","=================================================================\n","Total params: 14,714,688\n","Trainable params: 14,714,688\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0N9vIYl0Nt0S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619587822392,"user_tz":-540,"elapsed":772,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"9b614a67-f34b-4441-f4f2-3d48b3d01b84"},"source":["# network(모델)을 구성하는 layer들을 추출\n","\n","layers = cb.layers\n","type(layers), len(layers)  # model을 구성하는 layer들을 추출해서 list에 묶어서 반환"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(list, 19)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"Er2laF7rNt0T","executionInfo":{"status":"ok","timestamp":1619588022554,"user_tz":-540,"elapsed":864,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}}},"source":["layers[2].trainable = False"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"l_-paCUJNt0T","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1619588052358,"user_tz":-540,"elapsed":733,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"06dc7d69-e612-4520-81bc-c7f4ef9f1490"},"source":["# layer의 이름을 조회\n","layers[2].name"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'block1_conv2'"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"MFaBop_5Nt0T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619588248951,"user_tz":-540,"elapsed":767,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"6c27fa0d-c683-4645-c306-d10130172d1e"},"source":["# 모델.get_layer('layer이름'): 지정한 이름의 layer를 반환\n","l = cb.get_layer('block1_conv2')\n","l"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f049b625510>"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PGLFJGjk-e-f","executionInfo":{"status":"ok","timestamp":1619588249418,"user_tz":-540,"elapsed":735,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"4da0e728-35b0-4155-e98e-4b26449d6127"},"source":["# layer의 가중치(weights)를 조회 - layer객체.weights\n","l_w = l.weights\n","type(l_w), len(l_w)  # [weight, bias]의 형태로 반환"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(list, 2)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6kGDbiXy-e8S","executionInfo":{"status":"ok","timestamp":1619588453656,"user_tz":-540,"elapsed":796,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"85cd0d12-d64e-46cb-e9b4-1b5204c35a88"},"source":["np.shape(l_w[0]), np.shape(l_w[1])"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([3, 3, 64, 64]), TensorShape([64]))"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"hziFS9AFCq37","executionInfo":{"status":"ok","timestamp":1619589250233,"user_tz":-540,"elapsed":795,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}}},"source":["del layers"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7S9_K2RPNt0T"},"source":["### Pretrained 모델 Bottom layer들(Input과 가까운 Layer들)은 고정시키고 Top layer의 일부를 재학습"]},{"cell_type":"markdown","metadata":{"id":"ekYVAvVyNt0T"},"source":["- Conv_base에서 가장 Top부분에 있는 레이어에 대해 fine-tuning.\n","    - 앞의 layer들은 비교적 일반적이고 재사용 가능한 feature를 학습\n","    - 너무 많은 parameter를 학습시키면 overfitting의 위험이 있음 (특히 새로운 데이터의 수가 적을 때)"]},{"cell_type":"code","metadata":{"id":"_SmiGUYONt0T","executionInfo":{"status":"ok","timestamp":1619589485644,"user_tz":-540,"elapsed":1118,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}}},"source":["from tensorflow.keras import layers\n","\n","def create_model():\n","    # VGG16: block5_conv2, block5_conv3 두 개의 convolution layer들을 fine tuning\n","    conv_base = VGG16(weights='imagenet',\n","                      include_top=False,\n","                      input_shape=(150, 150, 3)\n","                      )\n","    # trainable 설정\n","    for layer in conv_base.layers:\n","        is_trainable = False\n","        if layer.name == 'block5_conv2' or layer.name == 'block5_conv3':\n","            is_trainable = True\n","        layer.trainable = is_trainable\n","    \n","    model = keras.Sequential()\n","    model.add(conv_base)\n","    model.add(layers.GlobalAveragePooling2D())\n","    model.add(layers.Dense(256, activation='relu'))\n","    model.add(layers.Dense(1, activation='sigmoid'))\n","\n","    return model"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"k56ABxb5Nt0T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619589486118,"user_tz":-540,"elapsed":1382,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"34ba1dce-fb7d-406f-bbcf-bfdc76bcba38"},"source":["model = create_model()\n","model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n","              loss='binary_crossentropy',\n","              metrics=['accuracy']\n","              )\n","model.summary()"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","vgg16 (Functional)           (None, 4, 4, 512)         14714688  \n","_________________________________________________________________\n","global_average_pooling2d_2 ( (None, 512)               0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 256)               131328    \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 1)                 257       \n","=================================================================\n","Total params: 14,846,273\n","Trainable params: 4,851,201\n","Non-trainable params: 9,995,072\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JJLqUV98Nt0T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619589564653,"user_tz":-540,"elapsed":1219,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"16157663-f080-4e96-c8b7-f6e3c96a7c3b"},"source":["train_iterator, validation_iterator, test_iterator = get_generators()"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Found 2000 images belonging to 2 classes.\n","Found 1000 images belonging to 2 classes.\n","Found 1000 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XpVRCeBONt0U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619590446479,"user_tz":-540,"elapsed":706607,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"823fe507-97b3-4a91-f8b4-368fb6ffc4e7"},"source":["N_EPOCHS = 30\n","mc_callback = keras.callbacks.ModelCheckpoint('./models/cat_dog_model', \n","                                              monitor='val_loss',\n","                                              save_best_only=True\n","                                              )\n","model.fit(train_iterator,\n","          epochs=N_EPOCHS,\n","          steps_per_epoch=len(train_iterator),\n","          validation_data=validation_iterator,\n","          validation_steps=len(validation_iterator),\n","          callbacks=[mc_callback]\n","          )"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n","20/20 [==============================] - 61s 986ms/step - loss: 1.0580 - accuracy: 0.5082 - val_loss: 0.4577 - val_accuracy: 0.7760\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 2/30\n","20/20 [==============================] - 20s 977ms/step - loss: 0.4480 - accuracy: 0.7864 - val_loss: 0.3245 - val_accuracy: 0.8520\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 3/30\n","20/20 [==============================] - 20s 983ms/step - loss: 0.3922 - accuracy: 0.8274 - val_loss: 0.2217 - val_accuracy: 0.9160\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 4/30\n","20/20 [==============================] - 20s 981ms/step - loss: 0.2862 - accuracy: 0.8774 - val_loss: 0.2574 - val_accuracy: 0.8880\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 5/30\n","20/20 [==============================] - 20s 983ms/step - loss: 0.2550 - accuracy: 0.8980 - val_loss: 0.2173 - val_accuracy: 0.9150\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 6/30\n","20/20 [==============================] - 20s 979ms/step - loss: 0.2417 - accuracy: 0.9065 - val_loss: 0.2072 - val_accuracy: 0.9170\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 7/30\n","20/20 [==============================] - 20s 976ms/step - loss: 0.1981 - accuracy: 0.9162 - val_loss: 0.1922 - val_accuracy: 0.9240\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 8/30\n","20/20 [==============================] - 20s 996ms/step - loss: 0.2079 - accuracy: 0.9235 - val_loss: 0.1929 - val_accuracy: 0.9240\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 9/30\n","20/20 [==============================] - 20s 981ms/step - loss: 0.2184 - accuracy: 0.9117 - val_loss: 0.2010 - val_accuracy: 0.9140\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 10/30\n","20/20 [==============================] - 20s 979ms/step - loss: 0.1692 - accuracy: 0.9372 - val_loss: 0.1834 - val_accuracy: 0.9230\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 11/30\n","20/20 [==============================] - 20s 992ms/step - loss: 0.1848 - accuracy: 0.9201 - val_loss: 0.1947 - val_accuracy: 0.9330\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 12/30\n","20/20 [==============================] - 20s 982ms/step - loss: 0.1534 - accuracy: 0.9472 - val_loss: 0.2106 - val_accuracy: 0.9130\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 13/30\n","20/20 [==============================] - 20s 977ms/step - loss: 0.2076 - accuracy: 0.9142 - val_loss: 0.2060 - val_accuracy: 0.9080\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 14/30\n","20/20 [==============================] - 20s 987ms/step - loss: 0.1867 - accuracy: 0.9203 - val_loss: 0.1843 - val_accuracy: 0.9310\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 15/30\n","20/20 [==============================] - 20s 984ms/step - loss: 0.1326 - accuracy: 0.9477 - val_loss: 0.1657 - val_accuracy: 0.9330\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 16/30\n","20/20 [==============================] - 20s 984ms/step - loss: 0.1112 - accuracy: 0.9626 - val_loss: 0.2199 - val_accuracy: 0.9180\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 17/30\n","20/20 [==============================] - 20s 985ms/step - loss: 0.1274 - accuracy: 0.9489 - val_loss: 0.2044 - val_accuracy: 0.9200\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 18/30\n","20/20 [==============================] - 20s 979ms/step - loss: 0.1433 - accuracy: 0.9437 - val_loss: 0.1807 - val_accuracy: 0.9330\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 19/30\n","20/20 [==============================] - 20s 986ms/step - loss: 0.1139 - accuracy: 0.9598 - val_loss: 0.2080 - val_accuracy: 0.9280\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 20/30\n","20/20 [==============================] - 20s 994ms/step - loss: 0.0974 - accuracy: 0.9597 - val_loss: 0.1827 - val_accuracy: 0.9190\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 21/30\n","20/20 [==============================] - 20s 1s/step - loss: 0.1190 - accuracy: 0.9551 - val_loss: 0.1980 - val_accuracy: 0.9300\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 22/30\n","20/20 [==============================] - 20s 1s/step - loss: 0.0959 - accuracy: 0.9623 - val_loss: 0.2326 - val_accuracy: 0.9230\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 23/30\n","20/20 [==============================] - 20s 995ms/step - loss: 0.1025 - accuracy: 0.9567 - val_loss: 0.2324 - val_accuracy: 0.9240\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 24/30\n","20/20 [==============================] - 20s 989ms/step - loss: 0.0896 - accuracy: 0.9650 - val_loss: 0.1867 - val_accuracy: 0.9370\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 25/30\n","20/20 [==============================] - 20s 984ms/step - loss: 0.0849 - accuracy: 0.9665 - val_loss: 0.1800 - val_accuracy: 0.9350\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 26/30\n","20/20 [==============================] - 20s 989ms/step - loss: 0.0932 - accuracy: 0.9605 - val_loss: 0.1997 - val_accuracy: 0.9300\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 27/30\n","20/20 [==============================] - 20s 989ms/step - loss: 0.1000 - accuracy: 0.9534 - val_loss: 0.2215 - val_accuracy: 0.9240\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 28/30\n","20/20 [==============================] - 20s 981ms/step - loss: 0.0845 - accuracy: 0.9653 - val_loss: 0.2039 - val_accuracy: 0.9360\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 29/30\n","20/20 [==============================] - 20s 984ms/step - loss: 0.0813 - accuracy: 0.9724 - val_loss: 0.2007 - val_accuracy: 0.9290\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n","Epoch 30/30\n","20/20 [==============================] - 20s 986ms/step - loss: 0.0539 - accuracy: 0.9792 - val_loss: 0.2303 - val_accuracy: 0.9320\n","INFO:tensorflow:Assets written to: ./models/cat_dog_model/assets\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f042c3e9610>"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"BhXhPUHPNt0U","executionInfo":{"status":"ok","timestamp":1619590984233,"user_tz":-540,"elapsed":2060,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}}},"source":["best_model = keras.models.load_model('./models/cat_dog_model')"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"7wl2PrEVNt0U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619591034320,"user_tz":-540,"elapsed":18082,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"bd6f5e50-b400-4a26-b2f9-87aa8512d758"},"source":["# evaluation\n","best_model.evaluate(train_iterator)"],"execution_count":32,"outputs":[{"output_type":"stream","text":["20/20 [==============================] - 17s 813ms/step - loss: 0.0606 - accuracy: 0.9750\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.06063142791390419, 0.9750000238418579]"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"VPIwN2uvNt0U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619591058284,"user_tz":-540,"elapsed":4058,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"bcac9eb5-8422-4eb7-fe0d-0316845b642c"},"source":["best_model.evaluate(test_iterator)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["10/10 [==============================] - 3s 306ms/step - loss: 0.2935 - accuracy: 0.9180\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.29349854588508606, 0.9179999828338623]"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"AAItHsB9Nt0U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619591120322,"user_tz":-540,"elapsed":1566,"user":{"displayName":"KIM TAEKYUN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLF5C4fFQDfiq1pEbiM8GO1_TQ5EX9tkNYvrmY8Q=s64","userId":"17041733956875075719"}},"outputId":"c074ffc3-d218-4163-bdf3-94366c4aea7a"},"source":["predict_cat_dog('/content/dog.jpg', model, mode=False)"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[8.74577e-06]], dtype=float32), array([[0]]), 'cat')"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"JdzY3K-DNt0U"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eYk_jWcqNt0U"},"source":[""],"execution_count":null,"outputs":[]}]}